# MCP Server Configuration
HOST=0.0.0.0
PORT=8051
MCP_SERVICE_PORT_HOST=8051 # Host port to map to the container's PORT for crawl4ai-mcp-service
TRANSPORT=sse # sse or stdio

# LiteLLM and Ollama Configuration
# URL of your remote FastAPI server that exposes Ollama models
# Example: OLLAMA_API_BASE_URL=http://my-custom-ollama-fastapi.internal:8000
# If Ollama is running directly and accessible: OLLAMA_API_BASE_URL=http://host.docker.internal:11434 (from Docker container to host)
# Or if Ollama is another Docker container on the same network: OLLAMA_API_BASE_URL=http://ollama_service_name:11434
OLLAMA_API_BASE_URL=http://10.44.44.13:11434 # Replace with your actual remote Ollama/FastAPI server URL

# Model for contextual generation (e.g., summarization, context extraction)
# This should be the model name as your Ollama/FastAPI server knows it.
# For LiteLLM with 'ollama/' prefix, it expects the part after 'ollama/'.
MODEL_CHOICE=llama3 # Example: llama3, mistral, etc. (LiteLLM will prefix with 'ollama/')

# Model for generating embeddings
# This should be the model name as your Ollama/FastAPI server knows it.
OLLAMA_EMBEDDING_MODEL=nomic-embed-text # Example: nomic-embed-text, mxbai-embed-large (LiteLLM will prefix with 'ollama/')

# Dimension of the embeddings produced by OLLAMA_EMBEDDING_MODEL
# nomic-embed-text: 768
# mxbai-embed-large: 1024
# Ensure this matches your chosen embedding model EXACTLY.
EMBEDDING_DIMENSION=768

# Qdrant Configuration
# QDRANT_URL will be http://qdrant:6333 because 'qdrant' is the service name in docker-compose.yml
QDRANT_URL=http://qdrant:6333
QDRANT_COLLECTION_NAME=crawled_content
# QDRANT_API_KEY=your-qdrant-api-key # Uncomment and set if your Qdrant instance uses an API key

# Python specific
PYTHONUNBUFFERED=1

# Logging (optional, if your app uses a configurable logger)
# LOG_LEVEL=INFO

# Crawl4AI specific (optional, if you need to override defaults)
# CRAWLAI_MAX_CONCURRENT_SESSIONS=5
# CRAWLAI_VERBOSE_LOGGING=False
